{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nilesh/work/dl_base\n"
     ]
    }
   ],
   "source": [
    "%cd /home/nilesh/work/dl_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time, socket, yaml, wandb, logging\n",
    "import logging.config\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nets import *\n",
    "from losses import *\n",
    "from optimizer_bundles import *\n",
    "from resources import _c, load_config_and_runtime_args, dump_diff_config\n",
    "from datasets import DATA_MANAGERS, XMCEvaluator\n",
    "from dl_helper import unwrap, expand_multilabel_dataset\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "transformers.set_seed(42)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config and runtime argument parsing\n",
    "args = load_config_and_runtime_args(\"python configs/seq2seq.yaml\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = 1e-3\n",
    "args.num_epochs = 100\n",
    "args.eval_topk = 5\n",
    "args.eval_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_gpu = 1\n",
    "args.device = 'cuda:3'\n",
    "args.use_grad_scaler = False\n",
    "args.hostname = socket.gethostname()\n",
    "args.exp_start_time = time.ctime()\n",
    "args.DATA_DIR = DATA_DIR = f'Datasets/{args.dataset}'\n",
    "args.OUT_DIR = OUT_DIR = f'Results/{args.project}/{args.dataset}/{args.expname}'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "torch.cuda.set_device(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs/logging.yaml') as f:\n",
    "    log_config = yaml.safe_load(f.read())\n",
    "    log_config['handlers']['file_handler']['filename'] = f\"{OUT_DIR}/{log_config['handlers']['file_handler']['filename']}\"\n",
    "    logging.config.dictConfig(log_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(project='Seq2Seq', expname='seq2seq-label-text', desc='Learn a sequence to sequence model to generate label text', dataset='EURLex-4K', net='t5', loss='tf-loss', data_manager='two-tower', tf='t5-small', tf_max_len=128, save=True, resume_path='', data_tokenization='offline', num_val_points=0, track_metric='nDCG@5', transpose_trn_dataset=False, optim_bundle='base', optim='adamw', num_epochs=50, dropout=0.5, warmup=0.1, bsz=1024, eval_interval=10, eval_topk=5, w_accumulation_steps=1, lr=0.002, weight_decay=0.01, amp_encode=False, norm_embs=False, use_swa=False, swa_start=8, swa_step=1000, num_gpu=1, device='cuda:3', use_grad_scaler=False, hostname='habanero.csres.utexas.edu', exp_start_time='Fri Oct  7 22:55:40 2022', DATA_DIR='Datasets/EURLex-4K', OUT_DIR='Results/Seq2Seq/EURLex-4K/seq2seq-label-text')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LOSSES[args.loss](args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "from datasets import TwoTowerDataset\n",
    "\n",
    "data_manager = DATA_MANAGERS[args.data_manager](args)\n",
    "trn_dataset, val_dataset, _ = data_manager.build_datasets()\n",
    "expand_multilabel_dataset(trn_dataset.x_dataset, copy=False, multiclass=True)\n",
    "data_manager.trn_dataset = TwoTowerDataset(trn_dataset.x_dataset, trn_dataset.y_dataset)\n",
    "\n",
    "trn_loader, val_loader, _ = data_manager.build_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = trn_dataset.x_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello my name is nilesh</s>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.decode(t1.encode_plus('hello my name is nilesh')['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decis ec european parliament council establish multiannu commun programm promot safer internet onlin technolog text eea relev european parliament council european union regard treati establish european commun articl thereof regard propos commiss regard opinion european econom social committe consult committe region act accord procedur laid articl treati internet penetr technolog mobil phone grow consider commun alongsid danger children abus technolog continu exist danger abus emerg order encourag exploit opportun offer internet onlin technolog mea</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.decode(trn_dataset.get_fts([0], 'x')['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers, pre_tokenizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace, Punctuation\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer, BpeTrainer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece, BPE\n",
    "\n",
    "label_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "label_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "label_tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation()])\n",
    "label_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"$A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")\n",
    "trainer = BpeTrainer(vocab_size=4000, special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[UNK]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y = [x.strip() for x in open(f'{DATA_DIR}/Y.txt')]\n",
    "label_tokenizer.train_from_iterator(Y, trainer)\n",
    "label_tokenizer.save(f'{OUT_DIR}/label_tokenizer.pt')\n",
    "label_tokens = label_tokenizer.encode_batch(Y)\n",
    "Y_ii = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(x.ids) for x in label_tokens], batch_first=True, padding_value=label_tokenizer.token_to_id('[PAD]'))\n",
    "data_manager.trn_dataset.y_dataset.X_ii = Y_ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "from dl_helper import BatchIterator\n",
    "\n",
    "from genre_trie import MarisaTrie\n",
    "Y = [x.strip() for x in open(f'{DATA_DIR}/Y.txt')]\n",
    "inv_Y = {v: i for i, v in enumerate(Y)}\n",
    "trie = MarisaTrie([val_dataset.get_fts([y], source='y')['input_ids'][0] for y in np.arange(len(val_dataset.y_dataset))], cache_fist_branch=False)\n",
    "\n",
    "def get_trie_candidates(batch_id, x):\n",
    "    return trie.get(x[1:])\n",
    "\n",
    "Y_size = label_tokenizer.get_vocab_size()\n",
    "def get_target_mask(seq, other_pos_seqs):\n",
    "    with torch.no_grad():\n",
    "        trie_mask = torch.zeros(len(seq), Y_size).bool()\n",
    "        target_mask = torch.ones(len(seq), Y_size).bool()\n",
    "        trie_mask.scatter_(1, seq.reshape(-1, 1), True)\n",
    "        for i in range(len(seq)):\n",
    "            candidates = np.array(trie.get(seq[:i]))\n",
    "            if len(candidates) < 2:\n",
    "                break\n",
    "            trie_mask[i, candidates] = True\n",
    "            trie_mask[i, other_pos_seqs[:, i]] = False\n",
    "            trie_mask[i, seq[i]] = True\n",
    "            \n",
    "            target_mask[i, np.intersect1d(other_pos_seqs[:, i], candidates)] = False\n",
    "            target_mask[i, seq[i]] = True\n",
    "        return trie_mask, target_mask\n",
    "    \n",
    "class T5(SWANet):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        tf_args = {'tie_word_embeddings': False}\n",
    "        self.tf = T5ForConditionalGeneration.from_pretrained(args.tf, **tf_args)\n",
    "\n",
    "    def forward(self, b):\n",
    "        b = self.ToD(b)\n",
    "        return self.tf(**b['xfts'], labels=b['yfts']['input_ids'])\n",
    "\n",
    "    def predict(self, data_loader, bsz=256, K=5):\n",
    "        self.eval()\n",
    "        data_iter = BatchIterator(data_loader.dataset.x_dataset, bsz)\n",
    "        out_seq, out_scores = [], []\n",
    "        with torch.no_grad():\n",
    "            for b in tqdm(data_iter):\n",
    "                b = self.ToD(b)\n",
    "                batch_out = self.tf.generate(\n",
    "                    **b['xfts'], \n",
    "                    num_return_sequences=K, \n",
    "                    num_beams=K, \n",
    "                    early_stopping=True, \n",
    "                    output_scores=True, \n",
    "                    return_dict_in_generate=True,\n",
    "                    prefix_allowed_tokens_fn=get_trie_candidates)\n",
    "                batch_out_seq, batch_out_scores = batch_out.sequences, batch_out.sequences_scores\n",
    "                out_seq.append(batch_out_seq.detach().cpu().numpy().reshape(-1, K, batch_out_seq.shape[1]))\n",
    "                out_scores.append(batch_out_scores.detach().cpu().numpy().reshape(-1, K))\n",
    "\n",
    "        out_lbls = [[inv_Y[''.join([x.strip() for x in label_tokenizer.decode(seq, skip_special_tokens=True)])] for instance_seq in batch_seq for seq in instance_seq] for batch_seq in out_seq]\n",
    "        data = np.concatenate(out_scores).flatten()\n",
    "        inds = np.concatenate(out_lbls)\n",
    "        indptr = np.arange(0, len(data)+1, K)\n",
    "        score_mat = sp.csr_matrix((data, inds, indptr), data_loader.dataset.labels.shape)\n",
    "        return score_mat\n",
    "        \n",
    "NETS['t5'] = T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  47,  522,  630, 1840, 1932, 2852], dtype=int32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_manager.trn_X_Y[0].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.41 ms ± 77.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ret = get_target_mask(Y_ii[47], Y_ii[[522,  630, 1840, 1932, 2852]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLoss(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.alpha = 5\n",
    "        self.lamda = 0.05\n",
    "\n",
    "    def forward(self, model, b):\n",
    "        out = model(b)\n",
    "        targets = torch.zeros_like(out.logits)\n",
    "        targets.scatter_(-1, b['yfts']['input_ids'].unsqueeze(-1), 1.0)\n",
    "        loss = nn.BCELoss()(out.logits, targets)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NETS[args.net](args)\n",
    "criterion = LOSSES[args.loss](args)\n",
    "evaluator = XMCEvaluator(args, val_loader, data_manager, 'val')\n",
    "optim_bundle = OPTIM_BUNDLES[args.optim_bundle](args)\n",
    "\n",
    "if os.path.exists(args.resume_path):\n",
    "    logging.info(f'Loading net state dict from: {args.resume_path}')\n",
    "    logging.info(net.load(args.resume_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.tf.lm_head = nn.Linear(net.tf.lm_head.in_features, label_tokenizer.get_vocab_size())\n",
    "\n",
    "net.tf.config.decoder_start_token_id = label_tokenizer.token_to_id('[CLS]')\n",
    "net.tf.config.eos_token_id = label_tokenizer.token_to_id('[SEP]')\n",
    "net.tf.config.pad_token_id = label_tokenizer.token_to_id('[PAD]')\n",
    "net.tf.decoder.config.vocab_size = label_tokenizer.get_vocab_size()\n",
    "\n",
    "net.tf.config.max_length = 32\n",
    "net.tf.config.min_length = 1\n",
    "net.tf.config.no_repeat_ngram_size = 3\n",
    "net.tf.config.early_stopping = True\n",
    "net.tf.config.length_penalty = 1.0\n",
    "net.tf.config.num_beams = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 22:55:53 : \u001b[1m\u001b[94m<class 'torch.optim.adamw.AdamW'>(base) ({'lr': 0.002}, accum: 1): \u001b[0m tf.shared.weight tf.encoder.block.0.layer.0.SelfAttention.q.weight tf.encoder.block.0.layer.0.SelfAttention.k.weight tf.encoder.block.0.layer.0.SelfAttention.v.weight tf.encoder.block.0.layer.0.SelfAttention.o.weight tf.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight tf.encoder.block.0.layer.0.layer_norm.weight tf.encoder.block.0.layer.1.DenseReluDense.wi.weight tf.encoder.block.0.layer.1.DenseReluDense.wo.weight tf.encoder.block.0.layer.1.layer_norm.weight tf.encoder.block.1.layer.0.SelfAttention.q.weight tf.encoder.block.1.layer.0.SelfAttention.k.weight tf.encoder.block.1.layer.0.SelfAttention.v.weight tf.encoder.block.1.layer.0.SelfAttention.o.weight tf.encoder.block.1.layer.0.layer_norm.weight tf.encoder.block.1.layer.1.DenseReluDense.wi.weight tf.encoder.block.1.layer.1.DenseReluDense.wo.weight tf.encoder.block.1.layer.1.layer_norm.weight tf.encoder.block.2.layer.0.SelfAttention.q.weight tf.encoder.block.2.layer.0.SelfAttention.k.weight tf.encoder.block.2.layer.0.SelfAttention.v.weight tf.encoder.block.2.layer.0.SelfAttention.o.weight tf.encoder.block.2.layer.0.layer_norm.weight tf.encoder.block.2.layer.1.DenseReluDense.wi.weight tf.encoder.block.2.layer.1.DenseReluDense.wo.weight tf.encoder.block.2.layer.1.layer_norm.weight tf.encoder.block.3.layer.0.SelfAttention.q.weight tf.encoder.block.3.layer.0.SelfAttention.k.weight tf.encoder.block.3.layer.0.SelfAttention.v.weight tf.encoder.block.3.layer.0.SelfAttention.o.weight tf.encoder.block.3.layer.0.layer_norm.weight tf.encoder.block.3.layer.1.DenseReluDense.wi.weight tf.encoder.block.3.layer.1.DenseReluDense.wo.weight tf.encoder.block.3.layer.1.layer_norm.weight tf.encoder.block.4.layer.0.SelfAttention.q.weight tf.encoder.block.4.layer.0.SelfAttention.k.weight tf.encoder.block.4.layer.0.SelfAttention.v.weight tf.encoder.block.4.layer.0.SelfAttention.o.weight tf.encoder.block.4.layer.0.layer_norm.weight tf.encoder.block.4.layer.1.DenseReluDense.wi.weight tf.encoder.block.4.layer.1.DenseReluDense.wo.weight tf.encoder.block.4.layer.1.layer_norm.weight tf.encoder.block.5.layer.0.SelfAttention.q.weight tf.encoder.block.5.layer.0.SelfAttention.k.weight tf.encoder.block.5.layer.0.SelfAttention.v.weight tf.encoder.block.5.layer.0.SelfAttention.o.weight tf.encoder.block.5.layer.0.layer_norm.weight tf.encoder.block.5.layer.1.DenseReluDense.wi.weight tf.encoder.block.5.layer.1.DenseReluDense.wo.weight tf.encoder.block.5.layer.1.layer_norm.weight tf.encoder.final_layer_norm.weight tf.decoder.block.0.layer.0.SelfAttention.q.weight tf.decoder.block.0.layer.0.SelfAttention.k.weight tf.decoder.block.0.layer.0.SelfAttention.v.weight tf.decoder.block.0.layer.0.SelfAttention.o.weight tf.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight tf.decoder.block.0.layer.0.layer_norm.weight tf.decoder.block.0.layer.1.EncDecAttention.q.weight tf.decoder.block.0.layer.1.EncDecAttention.k.weight tf.decoder.block.0.layer.1.EncDecAttention.v.weight tf.decoder.block.0.layer.1.EncDecAttention.o.weight tf.decoder.block.0.layer.1.layer_norm.weight tf.decoder.block.0.layer.2.DenseReluDense.wi.weight tf.decoder.block.0.layer.2.DenseReluDense.wo.weight tf.decoder.block.0.layer.2.layer_norm.weight tf.decoder.block.1.layer.0.SelfAttention.q.weight tf.decoder.block.1.layer.0.SelfAttention.k.weight tf.decoder.block.1.layer.0.SelfAttention.v.weight tf.decoder.block.1.layer.0.SelfAttention.o.weight tf.decoder.block.1.layer.0.layer_norm.weight tf.decoder.block.1.layer.1.EncDecAttention.q.weight tf.decoder.block.1.layer.1.EncDecAttention.k.weight tf.decoder.block.1.layer.1.EncDecAttention.v.weight tf.decoder.block.1.layer.1.EncDecAttention.o.weight tf.decoder.block.1.layer.1.layer_norm.weight tf.decoder.block.1.layer.2.DenseReluDense.wi.weight tf.decoder.block.1.layer.2.DenseReluDense.wo.weight tf.decoder.block.1.layer.2.layer_norm.weight tf.decoder.block.2.layer.0.SelfAttention.q.weight tf.decoder.block.2.layer.0.SelfAttention.k.weight tf.decoder.block.2.layer.0.SelfAttention.v.weight tf.decoder.block.2.layer.0.SelfAttention.o.weight tf.decoder.block.2.layer.0.layer_norm.weight tf.decoder.block.2.layer.1.EncDecAttention.q.weight tf.decoder.block.2.layer.1.EncDecAttention.k.weight tf.decoder.block.2.layer.1.EncDecAttention.v.weight tf.decoder.block.2.layer.1.EncDecAttention.o.weight tf.decoder.block.2.layer.1.layer_norm.weight tf.decoder.block.2.layer.2.DenseReluDense.wi.weight tf.decoder.block.2.layer.2.DenseReluDense.wo.weight tf.decoder.block.2.layer.2.layer_norm.weight tf.decoder.block.3.layer.0.SelfAttention.q.weight tf.decoder.block.3.layer.0.SelfAttention.k.weight tf.decoder.block.3.layer.0.SelfAttention.v.weight tf.decoder.block.3.layer.0.SelfAttention.o.weight tf.decoder.block.3.layer.0.layer_norm.weight tf.decoder.block.3.layer.1.EncDecAttention.q.weight tf.decoder.block.3.layer.1.EncDecAttention.k.weight tf.decoder.block.3.layer.1.EncDecAttention.v.weight tf.decoder.block.3.layer.1.EncDecAttention.o.weight tf.decoder.block.3.layer.1.layer_norm.weight tf.decoder.block.3.layer.2.DenseReluDense.wi.weight tf.decoder.block.3.layer.2.DenseReluDense.wo.weight tf.decoder.block.3.layer.2.layer_norm.weight tf.decoder.block.4.layer.0.SelfAttention.q.weight tf.decoder.block.4.layer.0.SelfAttention.k.weight tf.decoder.block.4.layer.0.SelfAttention.v.weight tf.decoder.block.4.layer.0.SelfAttention.o.weight tf.decoder.block.4.layer.0.layer_norm.weight tf.decoder.block.4.layer.1.EncDecAttention.q.weight tf.decoder.block.4.layer.1.EncDecAttention.k.weight tf.decoder.block.4.layer.1.EncDecAttention.v.weight tf.decoder.block.4.layer.1.EncDecAttention.o.weight tf.decoder.block.4.layer.1.layer_norm.weight tf.decoder.block.4.layer.2.DenseReluDense.wi.weight tf.decoder.block.4.layer.2.DenseReluDense.wo.weight tf.decoder.block.4.layer.2.layer_norm.weight tf.decoder.block.5.layer.0.SelfAttention.q.weight tf.decoder.block.5.layer.0.SelfAttention.k.weight tf.decoder.block.5.layer.0.SelfAttention.v.weight tf.decoder.block.5.layer.0.SelfAttention.o.weight tf.decoder.block.5.layer.0.layer_norm.weight tf.decoder.block.5.layer.1.EncDecAttention.q.weight tf.decoder.block.5.layer.1.EncDecAttention.k.weight tf.decoder.block.5.layer.1.EncDecAttention.v.weight tf.decoder.block.5.layer.1.EncDecAttention.o.weight tf.decoder.block.5.layer.1.layer_norm.weight tf.decoder.block.5.layer.2.DenseReluDense.wi.weight tf.decoder.block.5.layer.2.DenseReluDense.wo.weight tf.decoder.block.5.layer.2.layer_norm.weight tf.decoder.final_layer_norm.weight tf.lm_head.weight tf.lm_head.bias\n"
     ]
    }
   ],
   "source": [
    "net.to('cuda')\n",
    "optim_bundle.inject_params(net)\n",
    "optim_bundle.init_schedulers(args, len(trn_loader))\n",
    "logging.info(optim_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in trn_loader:\n",
    "    b = net.ToD(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4331e+00, -4.6934e+00,  1.5489e+00,  ...,  4.9096e-01,\n",
       "          -4.0118e+00, -2.9179e+00],\n",
       "         [ 2.1655e+00, -5.0546e+00,  2.1445e+01,  ..., -5.3368e+00,\n",
       "          -7.2699e+00, -1.1005e+00],\n",
       "         [ 2.0349e+01, -5.9286e+00, -2.6120e-01,  ..., -6.4735e+00,\n",
       "          -5.6312e+00, -1.0281e+00],\n",
       "         [ 2.0478e+01, -5.9548e+00, -3.7392e-02,  ..., -6.6003e+00,\n",
       "          -5.6674e+00, -9.5759e-01],\n",
       "         [ 2.0481e+01, -5.9625e+00, -5.5562e-02,  ..., -6.5869e+00,\n",
       "          -5.6471e+00, -9.7824e-01],\n",
       "         [ 2.0472e+01, -5.9733e+00, -1.7975e-02,  ..., -6.5771e+00,\n",
       "          -5.6551e+00, -9.9055e-01]],\n",
       "\n",
       "        [[-1.8961e+00, -3.0166e+00,  6.1340e-02,  ..., -1.1566e-01,\n",
       "          -4.3922e+00, -2.4037e+00],\n",
       "         [ 3.0239e+00, -5.5895e+00,  2.2094e+01,  ..., -5.8928e+00,\n",
       "          -7.8450e+00, -1.0512e+00],\n",
       "         [ 2.0453e+01, -6.1208e+00, -6.0008e-02,  ..., -6.3928e+00,\n",
       "          -5.7926e+00, -1.0604e+00],\n",
       "         [ 2.0574e+01, -6.1919e+00,  6.4543e-02,  ..., -6.6288e+00,\n",
       "          -5.8129e+00, -1.0533e+00],\n",
       "         [ 2.0572e+01, -6.1991e+00,  6.9133e-02,  ..., -6.6413e+00,\n",
       "          -5.8082e+00, -1.0678e+00],\n",
       "         [ 2.0562e+01, -6.2115e+00,  1.2630e-01,  ..., -6.6497e+00,\n",
       "          -5.8268e+00, -1.0710e+00]],\n",
       "\n",
       "        [[-6.3569e-01, -2.9122e+00,  2.5217e-01,  ...,  3.6654e+00,\n",
       "          -1.4584e+00, -1.2650e+00],\n",
       "         [ 1.6945e+00, -5.2997e+00,  2.2123e+01,  ..., -5.1111e+00,\n",
       "          -7.4277e+00, -9.5675e-01],\n",
       "         [ 2.0437e+01, -5.8836e+00, -4.0171e-01,  ..., -5.6722e+00,\n",
       "          -5.3250e+00, -1.0274e+00],\n",
       "         [ 2.0534e+01, -5.8413e+00, -3.2659e-01,  ..., -5.8875e+00,\n",
       "          -5.3460e+00, -9.4488e-01],\n",
       "         [ 2.0531e+01, -5.8186e+00, -4.0465e-01,  ..., -5.8203e+00,\n",
       "          -5.3259e+00, -9.5999e-01],\n",
       "         [ 2.0527e+01, -5.8196e+00, -4.0698e-01,  ..., -5.7773e+00,\n",
       "          -5.3251e+00, -9.7644e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 9.5403e-01, -5.8447e+00,  1.0046e+00,  ..., -3.2279e+00,\n",
       "          -2.5781e+00, -2.0750e+00],\n",
       "         [ 3.1678e+00, -5.8029e+00,  2.1640e+01,  ..., -5.7944e+00,\n",
       "          -7.6593e+00, -1.7451e+00],\n",
       "         [ 2.0389e+01, -6.1805e+00, -5.7112e-02,  ..., -6.6852e+00,\n",
       "          -5.5729e+00, -1.1135e+00],\n",
       "         [ 2.0498e+01, -6.2091e+00,  1.0353e-01,  ..., -6.8566e+00,\n",
       "          -5.5896e+00, -1.1474e+00],\n",
       "         [ 2.0501e+01, -6.2125e+00,  1.2483e-01,  ..., -6.8637e+00,\n",
       "          -5.5855e+00, -1.1579e+00],\n",
       "         [ 2.0498e+01, -6.2269e+00,  1.7681e-01,  ..., -6.8681e+00,\n",
       "          -5.6011e+00, -1.1590e+00]],\n",
       "\n",
       "        [[-3.9140e-01, -2.8922e+00, -1.8142e+00,  ...,  2.2489e+00,\n",
       "          -2.0309e+00, -1.2287e+00],\n",
       "         [ 2.6111e+00, -5.3529e+00,  2.1822e+01,  ..., -5.5918e+00,\n",
       "          -7.3045e+00, -1.3067e+00],\n",
       "         [ 2.0488e+01, -5.9703e+00, -6.6656e-02,  ..., -6.2959e+00,\n",
       "          -5.5194e+00, -1.0947e+00],\n",
       "         [ 2.0593e+01, -6.0369e+00, -8.3528e-02,  ..., -6.5160e+00,\n",
       "          -5.5385e+00, -1.0647e+00],\n",
       "         [ 2.0582e+01, -6.0338e+00, -5.4278e-02,  ..., -6.5195e+00,\n",
       "          -5.5291e+00, -1.0770e+00],\n",
       "         [ 2.0570e+01, -6.0435e+00,  1.2914e-03,  ..., -6.5166e+00,\n",
       "          -5.5405e+00, -1.0860e+00]],\n",
       "\n",
       "        [[ 1.2920e+00, -4.4696e+00,  2.7874e+00,  ..., -4.0698e+00,\n",
       "          -9.1709e-01, -2.4480e+00],\n",
       "         [ 2.4002e+00, -5.6692e+00,  2.1867e+01,  ..., -5.9353e+00,\n",
       "          -7.4787e+00, -1.2714e+00],\n",
       "         [ 2.0486e+01, -6.1263e+00, -1.0095e-01,  ..., -6.5362e+00,\n",
       "          -5.6061e+00, -1.1682e+00],\n",
       "         [ 2.0572e+01, -6.1025e+00,  1.1668e-01,  ..., -6.6366e+00,\n",
       "          -5.5908e+00, -1.1484e+00],\n",
       "         [ 2.0569e+01, -6.0981e+00,  1.2867e-01,  ..., -6.6352e+00,\n",
       "          -5.5804e+00, -1.1582e+00],\n",
       "         [ 2.0563e+01, -6.1071e+00,  1.7820e-01,  ..., -6.6410e+00,\n",
       "          -5.5862e+00, -1.1588e+00]]], device='cuda:3', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/50, Loss: 6.0472E+00: 100%|██████████| 81/81 [00:42<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 22:56:36 : Mean loss after epoch 0/50: 6.0472E+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/nilesh/miniconda3/lib/python3.9/site-packages/transformers/generation_utils.py:1764: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n",
      "100%|██████████| 4/4 [01:06<00:00, 16.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mFound new best model with nDCG@5: 0.79\n",
      "\u001b[0m\n",
      "INFO - root - 07-Oct-22 22:57:43 : \n",
      "P@1\tP@3\tP@5\tnDCG@1\tnDCG@3\tnDCG@5\tPSP@1\tPSP@3\tPSP@5\tR@10\tR@20\tR@50\tMRR@10\tloss\n",
      "0.23\t1.08\t0.78\t0.23\t0.93\t0.79\t0.15\t0.58\t0.55\t0.73\t0.73\t0.73\t1.71\t6.0472E+00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, Loss: 1.2081E+00: 100%|██████████| 81/81 [00:41<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 22:58:25 : Mean loss after epoch 1/50: 1.2081E+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2/50, Loss: 9.1413E-01: 100%|██████████| 81/81 [00:41<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 22:59:06 : Mean loss after epoch 2/50: 9.1413E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3/50, Loss: 7.2249E-01: 100%|██████████| 81/81 [00:41<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 22:59:48 : Mean loss after epoch 3/50: 7.2249E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4/50, Loss: 5.5545E-01: 100%|██████████| 81/81 [00:41<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:00:30 : Mean loss after epoch 4/50: 5.5545E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5/50, Loss: 4.1543E-01: 100%|██████████| 81/81 [00:41<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:01:12 : Mean loss after epoch 5/50: 4.1543E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6/50, Loss: 3.5508E-01: 100%|██████████| 81/81 [00:42<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:01:54 : Mean loss after epoch 6/50: 3.5508E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7/50, Loss: 3.2903E-01: 100%|██████████| 81/81 [00:41<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:02:35 : Mean loss after epoch 7/50: 3.2903E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8/50, Loss: 3.0653E-01: 100%|██████████| 81/81 [00:41<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:03:17 : Mean loss after epoch 8/50: 3.0653E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9/50, Loss: 2.8784E-01: 100%|██████████| 81/81 [00:41<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:03:59 : Mean loss after epoch 9/50: 2.8784E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10/50, Loss: 2.6414E-01: 100%|██████████| 81/81 [00:41<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:04:40 : Mean loss after epoch 10/50: 2.6414E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/nilesh/miniconda3/lib/python3.9/site-packages/transformers/generation_utils.py:1764: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n",
      "100%|██████████| 4/4 [01:39<00:00, 24.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mFound new best model with nDCG@5: 53.03\n",
      "\u001b[0m\n",
      "INFO - root - 07-Oct-22 23:06:21 : \n",
      "P@1\tP@3\tP@5\tnDCG@1\tnDCG@3\tnDCG@5\tPSP@1\tPSP@3\tPSP@5\tR@10\tR@20\tR@50\tMRR@10\tloss\n",
      "67.68\t54.97\t45.07\t67.68\t58.15\t53.03\t33.32\t36.84\t38.45\t44.21\t44.21\t44.21\t78.56\t2.6414E-01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 11/50, Loss: 2.6203E-01: 100%|██████████| 81/81 [00:41<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:07:02 : Mean loss after epoch 11/50: 2.6203E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 12/50, Loss: 2.5555E-01: 100%|██████████| 81/81 [00:41<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:07:43 : Mean loss after epoch 12/50: 2.5555E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 13/50, Loss: 2.4944E-01: 100%|██████████| 81/81 [00:41<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:08:25 : Mean loss after epoch 13/50: 2.4944E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 14/50, Loss: 2.4497E-01: 100%|██████████| 81/81 [00:41<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:09:07 : Mean loss after epoch 14/50: 2.4497E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 15/50, Loss: 2.3911E-01: 100%|██████████| 81/81 [00:41<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:09:49 : Mean loss after epoch 15/50: 2.3911E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 16/50, Loss: 2.2786E-01: 100%|██████████| 81/81 [00:41<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:10:30 : Mean loss after epoch 16/50: 2.2786E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 17/50, Loss: 2.1851E-01: 100%|██████████| 81/81 [00:41<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:11:12 : Mean loss after epoch 17/50: 2.1851E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 18/50, Loss: 2.1961E-01: 100%|██████████| 81/81 [00:41<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:11:54 : Mean loss after epoch 18/50: 2.1961E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 19/50, Loss: 2.1731E-01: 100%|██████████| 81/81 [00:41<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:12:35 : Mean loss after epoch 19/50: 2.1731E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 20/50, Loss: 2.1251E-01: 100%|██████████| 81/81 [00:41<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - root - 07-Oct-22 23:13:17 : Mean loss after epoch 20/50: 2.1251E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/nilesh/miniconda3/lib/python3.9/site-packages/transformers/generation_utils.py:1764: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n",
      "100%|██████████| 4/4 [02:17<00:00, 34.46s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'translation_centre_for_the_bodies_of_the))))))))))))))))))'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/nilesh/work/dl_base/scratchpad.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m epoch_loss \u001b[39m=\u001b[39m (epoch_loss\u001b[39m/\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMean loss after epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m.\u001b[39mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m%.4E\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m(epoch_loss)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m metrics \u001b[39m=\u001b[39m evaluator\u001b[39m.\u001b[39;49mpredict_and_track_eval(unwrap(net), epoch, epoch_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m metrics \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mmetrics\u001b[39m.\u001b[39mto_csv(sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/work/dl_base/datasets.py:322\u001b[0m, in \u001b[0;36mXMCEvaluator.predict_and_track_eval\u001b[0;34m(self, net, epoch, loss)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_and_track_eval\u001b[39m(\u001b[39mself\u001b[39m, net, epoch\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m    321\u001b[0m \t\u001b[39mif\u001b[39;00m epoch\u001b[39m%\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m epoch \u001b[39m==\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_epochs\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 322\u001b[0m \t\tscore_mat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(net)\n\u001b[1;32m    323\u001b[0m \t\tmetrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval(score_mat, epoch, loss)\n\u001b[1;32m    324\u001b[0m \t\t\u001b[39mif\u001b[39;00m metrics\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_metric] \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_score:\n",
      "File \u001b[0;32m~/work/dl_base/datasets.py:308\u001b[0m, in \u001b[0;36mXMCEvaluator.predict\u001b[0;34m(self, net)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, net):\n\u001b[0;32m--> 308\u001b[0m \tscore_mat \u001b[39m=\u001b[39m unwrap(net)\u001b[39m.\u001b[39;49mpredict(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_source, K\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_topk, bsz\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbsz)\n\u001b[1;32m    309\u001b[0m \t\u001b[39mreturn\u001b[39;00m score_mat\n",
      "\u001b[1;32m/home/nilesh/work/dl_base/scratchpad.ipynb Cell 19\u001b[0m in \u001b[0;36mT5.predict\u001b[0;34m(self, data_loader, bsz, K)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m         out_seq\u001b[39m.\u001b[39mappend(batch_out_seq\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, K, batch_out_seq\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m         out_scores\u001b[39m.\u001b[39mappend(batch_out_scores\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, K))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m out_lbls \u001b[39m=\u001b[39m [[inv_Y[\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([x\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m label_tokenizer\u001b[39m.\u001b[39mdecode(seq, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)])] \u001b[39mfor\u001b[39;00m instance_seq \u001b[39min\u001b[39;00m batch_seq \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m instance_seq] \u001b[39mfor\u001b[39;00m batch_seq \u001b[39min\u001b[39;00m out_seq]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(out_scores)\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m inds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(out_lbls)\n",
      "\u001b[1;32m/home/nilesh/work/dl_base/scratchpad.ipynb Cell 19\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m         out_seq\u001b[39m.\u001b[39mappend(batch_out_seq\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, K, batch_out_seq\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m         out_scores\u001b[39m.\u001b[39mappend(batch_out_scores\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, K))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m out_lbls \u001b[39m=\u001b[39m [[inv_Y[\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([x\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m label_tokenizer\u001b[39m.\u001b[39mdecode(seq, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)])] \u001b[39mfor\u001b[39;00m instance_seq \u001b[39min\u001b[39;00m batch_seq \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m instance_seq] \u001b[39mfor\u001b[39;00m batch_seq \u001b[39min\u001b[39;00m out_seq]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(out_scores)\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m inds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(out_lbls)\n",
      "\u001b[1;32m/home/nilesh/work/dl_base/scratchpad.ipynb Cell 19\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m         out_seq\u001b[39m.\u001b[39mappend(batch_out_seq\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, K, batch_out_seq\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m         out_scores\u001b[39m.\u001b[39mappend(batch_out_scores\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, K))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m out_lbls \u001b[39m=\u001b[39m [[inv_Y[\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin([x\u001b[39m.\u001b[39;49mstrip() \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m label_tokenizer\u001b[39m.\u001b[39;49mdecode(seq, skip_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)])] \u001b[39mfor\u001b[39;00m instance_seq \u001b[39min\u001b[39;00m batch_seq \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m instance_seq] \u001b[39mfor\u001b[39;00m batch_seq \u001b[39min\u001b[39;00m out_seq]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(out_scores)\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhabanero.csres.utexas.edu/home/nilesh/work/dl_base/scratchpad.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m inds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(out_lbls)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'translation_centre_for_the_bodies_of_the))))))))))))))))))'"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_loss = 0\n",
    "    net.train()\n",
    "    optim_bundle.zero_grad()\n",
    "\n",
    "    t = tqdm(trn_loader, desc='Epoch: 0, Loss: 0.0', leave=True)\n",
    "    for i, b in enumerate(t):\n",
    "        loss = criterion(net, b)\n",
    "        loss.backward()\n",
    "        \n",
    "        optim_bundle.step_and_zero_grad(scaler if args.use_grad_scaler else None)\n",
    "        unwrap(net).update_non_parameters(epoch, global_step)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        global_step += 1\n",
    "        t.set_description('Epoch: %d/%d, Loss: %.4E'%(epoch, args.num_epochs, (epoch_loss/(i+1))), refresh=True)\n",
    "    \n",
    "    epoch_loss = (epoch_loss/(i+1))\n",
    "    logging.info(f'Mean loss after epoch {epoch}/{args.num_epochs}: {\"%.4E\"%(epoch_loss)}')\n",
    "    metrics = evaluator.predict_and_track_eval(unwrap(net), epoch, epoch_loss)\n",
    "    if metrics is not None:\n",
    "        logging.info('\\n'+metrics.to_csv(sep='\\t', index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save(f'{OUT_DIR}/model.pt')\n",
    "# net.load(f'{OUT_DIR}/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnz = data_manager.trn_X_Y.getnnz(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mat = sp.load_npz(f'{OUT_DIR}/val_score_mat.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>nDCG@1</th>\n",
       "      <th>nDCG@3</th>\n",
       "      <th>nDCG@5</th>\n",
       "      <th>PSP@1</th>\n",
       "      <th>PSP@3</th>\n",
       "      <th>PSP@5</th>\n",
       "      <th>R@10</th>\n",
       "      <th>R@20</th>\n",
       "      <th>R@50</th>\n",
       "      <th>MRR@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <td>75.58</td>\n",
       "      <td>68.54</td>\n",
       "      <td>59.06</td>\n",
       "      <td>75.58</td>\n",
       "      <td>70.62</td>\n",
       "      <td>66.79</td>\n",
       "      <td>40.18</td>\n",
       "      <td>48.98</td>\n",
       "      <td>53.5</td>\n",
       "      <td>57.96</td>\n",
       "      <td>57.96</td>\n",
       "      <td>57.96</td>\n",
       "      <td>84.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          P@1    P@3    P@5  nDCG@1  nDCG@3  nDCG@5  PSP@1  PSP@3  PSP@5  \\\n",
       "Method  75.58  68.54  59.06   75.58   70.62   66.79  40.18  48.98   53.5   \n",
       "\n",
       "         R@10   R@20   R@50  MRR@10  \n",
       "Method  57.96  57.96  57.96   84.69  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from resources import compute_xmc_metrics\n",
    "K = 5\n",
    "compute_xmc_metrics(score_mat, val_dataset.labels, data_manager.inv_prop, K=K, disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[3092]: \u001b[1m \u001b[0m\n",
      "\n",
      "1) \u001b[33mindustrial_region\u001b[0m [1823] (-0.6737, 66)\n",
      "\n",
      "2) \u001b[33mcoordination_of_aid\u001b[0m [749] (-0.7332, 59)\n",
      "\n",
      "3) \u001b[33mregional_development\u001b[0m [2950] (-0.8451, 104)\n",
      "\n",
      "4) \u001b[33mregional_planning\u001b[0m [2955] (-0.9912, 76)\n",
      "\n",
      "5) \u001b[33mcommunity_financial_instrument\u001b[0m [629] (-1.0156, 198)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from resources import vis_point\n",
    "\n",
    "x = np.random.randint(score_mat.shape[0])\n",
    "vis_point(x, score_mat, [' ']*score_mat.shape[0], Y, nnz, val_dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 : \n",
      "\u001b[1m\u001b[4mexport_refund\u001b[0m(1.00, 1360) \u001b[1m\u001b[4mfishery_product\u001b[0m(1.00, 1465) \u001b[1m\u001b[4moriginating_product\u001b[0m(1.00, 2535) \u001b[1m\u001b[4mprocessed_foodstuff\u001b[0m(1.00, 2779) \u001b[1m\u001b[4mship's_flag\u001b[0m(1.00, 3169) \u001b[1m\u001b[4mthird_country\u001b[0m(1.00, 3475)\n",
      "\u001b[94mfishery _ product -0.46635327\u001b[0m, \u001b[94mexport _ refund -0.5364342\u001b[0m, \u001b[94mexport _ licence -0.92781454\u001b[0m, \u001b[94mimport _ policy -1.001939\u001b[0m, \u001b[94mmarketing _ standard -1.0298482\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "print(get_text(x, Y, val_loader.dataset.labels))\n",
    "print(*[_c(*x, attr='blue') for x in [(tokenizer.decode(out_seq[0][x, i], skip_special_tokens=True), out_scores[0][x, i]) for i in range(K)]], sep=', ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f09432f7b49e4950782662da773944188b91bdfaa0b96cc2296d6bd8357fad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
