# General parameters
project: 'Seq2Seq' # Project name
expname: 'seq2seq-label-text' # Experiment name
desc: 'Learn a sequence to sequence model to generate label text' # Short description of the exepriment
dataset: 'EURLex-4K' # Dataset name 
net: 't5' # Network class identifier
loss: 'tf-loss' # Loss class identifier
data_manager: two-tower
tf: 't5-small' # Name of transformer encoder to be used in the model, all huggingface transformer model names are applicable
tf_max_len: 128 # Maximum input sequence length of transformer
save: True # Save best model and score matrix
resume_path: '' # Resume training from this model checkpoint
data_tokenization: offline # Mode of data tokenization [offline/online]
num_val_points: 0 # Number of validation points taken from training set for evaluation during training
track_metric: nDCG@5 # Track this metric to evaluate best model
transpose_trn_dataset: False

# Network parameters

# Training parameters
optim_bundle: base # Optimizer bundle class identifier
optim: adamw # Torch optimizer class identifier
num_epochs: 50 # Number of epochs
dropout: 0.5 # Dropout on encoder embeddings
warmup: 0.1 # Fraction of warmup steps
bsz: 1024 # Mini-batch size
eval_interval: 3 # Evaluate current model at every specified number of epochs
eval_topk: 100 # Number of label predictions for each test point during evaluation
w_accumulation_steps: 1 # It's more efficient to apply weight updates to classification matrix after every accumulation steps
lr: 1.e-4 # Learning rate for the rest of the parameters
weight_decay: 0.01 # Optimizer weight decay
amp_encode: False # Encode input (i.e. transformer forward) using pytorch amp
norm_embs: False # Normalize embeddings
use_swa: False # Use stochastic weighted averaging (SWA) trick, can give minor improvements in results
swa_start: 8 # SWA start epoch
swa_step: 1000 # SWA step size