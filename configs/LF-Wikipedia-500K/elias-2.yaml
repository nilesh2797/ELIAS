
__dependency__: configs/elias.yaml

# General parameters
expname: 'ELIAS-2-e-50-loss_lamb_0.05-lr_clf_0.02-lr_enc_1e-4-bs_420-num_clusters_2048' # Experiment name
dataset: LF-AmazonTitles-131K # Dataset name 
net: elias-2 # Network class identifier
loss: elias-loss # Loss class identifier
tf_max_len: 32 # Maximum input sequence length of transformer
resume_path: Results/[project]/[dataset]/ELIAS-1-loss_lamb_0.05-lr_clf_0.02-lr_enc_1e-4-bs_512-num_clusters_2048/model.pt

# Network parameters
A_init_path: Results/[project]/[dataset]/ELIAS-1-loss_lamb_0.05-lr_clf_0.02-lr_enc_1e-4-bs_512-num_clusters_2048/A_approx.npz # Initialize label-cluster adjacency matrix from this file (expects *.npz file)
loss_lambda: 0.05

# Training parameters
num_epochs: 40 # Number of epochs
bsz: 420 # Mini-batch size
w_accumulation_steps: 10 # It's more efficient to apply weight updates to classification matrix after every specified accumulation steps
lr: 0.005 # Learning rate for network parameters
lr_wl: 0.02 # Learning rate for the classifier matrix (W_l) parameters
lr_tf: 1.0e-4 # Learning rate for the transformer encoder (\phi) parameters
max_leaf: 64 # 2048 clusters will be there
use_swa: True # Use stochastic weighted averaging (SWA) trick, can give minor improvements in results
swa_start: 8 # SWA start epoch
swa_step: 500 # SWA step size
